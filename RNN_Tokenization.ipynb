{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "sQeKVr0mFetg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Preparation\n",
        "text = \"hello world how are you today\""
      ],
      "metadata": {
        "id": "eM8jVsV5Fg_W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(char_level=True)"
      ],
      "metadata": {
        "id": "_Yr0x4lYFiWO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "DgUBZtX7FiIX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "BO8qvCb4FiAG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7pgH5l1Fhye",
        "outputId": "7178c12d-6a5e-4de1-e901-0dc8e7de9439"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'o': 1,\n",
              " ' ': 2,\n",
              " 'l': 3,\n",
              " 'h': 4,\n",
              " 'e': 5,\n",
              " 'w': 6,\n",
              " 'r': 7,\n",
              " 'd': 8,\n",
              " 'a': 9,\n",
              " 'y': 10,\n",
              " 'u': 11,\n",
              " 't': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = tokenizer.texts_to_sequences([text])[0]"
      ],
      "metadata": {
        "id": "Hu8VigIIFtEo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvN9_4sSFvUW",
        "outputId": "bbe1ca4c-78f0-4966-a99a-9dd8606f19d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4,\n",
              " 5,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 7,\n",
              " 3,\n",
              " 8,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 6,\n",
              " 2,\n",
              " 9,\n",
              " 7,\n",
              " 5,\n",
              " 2,\n",
              " 10,\n",
              " 1,\n",
              " 11,\n",
              " 2,\n",
              " 12,\n",
              " 1,\n",
              " 8,\n",
              " 9,\n",
              " 10]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input-target pairs\n",
        "# Example: \"hello\" -> \"ello \"\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "for i in range(1, len(encoded_text)):\n",
        "    input_sequences.append(encoded_text[:i])\n",
        "    target_sequences.append(encoded_text[i])"
      ],
      "metadata": {
        "id": "KVqSqSo-GW5H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLrj8m9zGZNX",
        "outputId": "29fc671c-bef0-400b-8470-950cb5204a83"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4],\n",
              " [4, 5],\n",
              " [4, 5, 3],\n",
              " [4, 5, 3, 3],\n",
              " [4, 5, 3, 3, 1],\n",
              " [4, 5, 3, 3, 1, 2],\n",
              " [4, 5, 3, 3, 1, 2, 6],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9, 7],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9, 7, 5],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9, 7, 5, 2],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9, 7, 5, 2, 10],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9, 7, 5, 2, 10, 1],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9, 7, 5, 2, 10, 1, 11],\n",
              " [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9, 7, 5, 2, 10, 1, 11, 2],\n",
              " [4,\n",
              "  5,\n",
              "  3,\n",
              "  3,\n",
              "  1,\n",
              "  2,\n",
              "  6,\n",
              "  1,\n",
              "  7,\n",
              "  3,\n",
              "  8,\n",
              "  2,\n",
              "  4,\n",
              "  1,\n",
              "  6,\n",
              "  2,\n",
              "  9,\n",
              "  7,\n",
              "  5,\n",
              "  2,\n",
              "  10,\n",
              "  1,\n",
              "  11,\n",
              "  2,\n",
              "  12],\n",
              " [4,\n",
              "  5,\n",
              "  3,\n",
              "  3,\n",
              "  1,\n",
              "  2,\n",
              "  6,\n",
              "  1,\n",
              "  7,\n",
              "  3,\n",
              "  8,\n",
              "  2,\n",
              "  4,\n",
              "  1,\n",
              "  6,\n",
              "  2,\n",
              "  9,\n",
              "  7,\n",
              "  5,\n",
              "  2,\n",
              "  10,\n",
              "  1,\n",
              "  11,\n",
              "  2,\n",
              "  12,\n",
              "  1],\n",
              " [4,\n",
              "  5,\n",
              "  3,\n",
              "  3,\n",
              "  1,\n",
              "  2,\n",
              "  6,\n",
              "  1,\n",
              "  7,\n",
              "  3,\n",
              "  8,\n",
              "  2,\n",
              "  4,\n",
              "  1,\n",
              "  6,\n",
              "  2,\n",
              "  9,\n",
              "  7,\n",
              "  5,\n",
              "  2,\n",
              "  10,\n",
              "  1,\n",
              "  11,\n",
              "  2,\n",
              "  12,\n",
              "  1,\n",
              "  8],\n",
              " [4,\n",
              "  5,\n",
              "  3,\n",
              "  3,\n",
              "  1,\n",
              "  2,\n",
              "  6,\n",
              "  1,\n",
              "  7,\n",
              "  3,\n",
              "  8,\n",
              "  2,\n",
              "  4,\n",
              "  1,\n",
              "  6,\n",
              "  2,\n",
              "  9,\n",
              "  7,\n",
              "  5,\n",
              "  2,\n",
              "  10,\n",
              "  1,\n",
              "  11,\n",
              "  2,\n",
              "  12,\n",
              "  1,\n",
              "  8,\n",
              "  9]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHECz3HfGaPX",
        "outputId": "4295e017-10d4-4495-f7f1-101fa212d78b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 7,\n",
              " 3,\n",
              " 8,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 6,\n",
              " 2,\n",
              " 9,\n",
              " 7,\n",
              " 5,\n",
              " 2,\n",
              " 10,\n",
              " 1,\n",
              " 11,\n",
              " 2,\n",
              " 12,\n",
              " 1,\n",
              " 8,\n",
              " 9,\n",
              " 10]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to a fixed length for batching\n",
        "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
        "padded_input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "target_sequences_one_hot = to_categorical(target_sequences, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "A9Y5uBUlGaKe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Model Definition\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 10, input_length=max_sequence_len),\n",
        "    SimpleRNN(50, return_sequences=False), # return_sequences=False for predicting single next token\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKaPw8uDGaH-",
        "outputId": "c00b273a-237f-497f-9858-b81a591e6a97"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Training\n",
        "model.fit(padded_input_sequences, target_sequences_one_hot, epochs=100, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5YFlcijGxMI",
        "outputId": "965aa7b0-46a0-4b27-b847-b916067a26d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x799f591110d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len=max_sequence_len\n",
        "seed_text = 'th'\n",
        "generated_text = seed_text\n",
        "encoded_seed = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "padded_seed = tf.keras.preprocessing.sequence.pad_sequences([encoded_seed], maxlen=max_seq_len, padding='pre')\n",
        "predicted_probs = model.predict(padded_seed, verbose=0)[0]"
      ],
      "metadata": {
        "id": "rTagGvN2GxJ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_seed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-fzk5SPGxHd",
        "outputId": "8d70b12d-f2c2-4a69-c144-33fac232a520"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0i5z4WyGw_W",
        "outputId": "11840b6f-655a-4d90-b522-ba133250edee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12,  4]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fnWdUhcG6x2",
        "outputId": "f2e17946-e168-4dcf-9197-8915c3a56b19"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00778203, 0.2669546 , 0.13166967, 0.33389673, 0.02348744,\n",
              "       0.0435743 , 0.08044758, 0.05700757, 0.03199705, 0.00979185,\n",
              "       0.00492762, 0.00398928, 0.00447428], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_token_index = np.argmax(predicted_probs)\n",
        "predicted_token_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTZHWUWRG6tf",
        "outputId": "2ea8805d-0643-4a93-9319-0ba6404fb6db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(3)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_char = tokenizer.index_word[predicted_token_index]\n",
        "predicted_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q9x-sJKQG6qX",
        "outputId": "7c33df0f-c37d-4a26-e8ac-850d0019dc39"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'l'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text += predicted_char\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ee_qvcbmG6n4",
        "outputId": "05347346-cba8-4a4d-9d9c-c8663ad7717a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text += predicted_char\n",
        "seed_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8hloaTOnHBgH",
        "outputId": "e55dcff9-f9b4-4c5f-ec36-96bf4a05ef3c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Generation\n",
        "def generate_text(model, tokenizer, seed_text, num_generate=10, max_seq_len=max_sequence_len):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(num_generate):\n",
        "        encoded_seed = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        padded_seed = tf.keras.preprocessing.sequence.pad_sequences([encoded_seed], maxlen=max_seq_len, padding='pre')\n",
        "        predicted_probs = model.predict(padded_seed, verbose=0)[0]\n",
        "        predicted_token_index = np.argmax(predicted_probs)\n",
        "        predicted_char = tokenizer.index_word[predicted_token_index]\n",
        "        generated_text += predicted_char\n",
        "        seed_text += predicted_char # Update seed for next prediction\n",
        "    return generated_text\n",
        "\n",
        "# Example generation\n",
        "generated_output = generate_text(model, tokenizer, \"h\", num_generate=15)\n",
        "print(f\"Generated text: {generated_output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrWq3DYCHBbG",
        "outputId": "01acc913-c0d0-46b8-8e60-6de7c010e3f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: hllloooolld how \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b74e2422",
        "outputId": "206eda46-8934-4bfb-d2a0-325fa178a6d5"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Assuming 'text' is already defined, e.g., text = \"hello world how are you today\"\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "encoded_text = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "print(\"Tokenizer word_index:\", tokenizer.word_index)\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "print(\"Encoded Text:\", encoded_text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer word_index: {'o': 1, ' ': 2, 'l': 3, 'h': 4, 'e': 5, 'w': 6, 'r': 7, 'd': 8, 'a': 9, 'y': 10, 'u': 11, 't': 12}\n",
            "Vocab Size: 13\n",
            "Encoded Text: [4, 5, 3, 3, 1, 2, 6, 1, 7, 3, 8, 2, 4, 1, 6, 2, 9, 7, 5, 2, 10, 1, 11, 2, 12, 1, 8, 9, 10]\n"
          ]
        }
      ]
    }
  ]
}